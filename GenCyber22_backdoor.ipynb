{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"backdoor.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XGBsH2E5EzqK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656535739862,"user_tz":240,"elapsed":19988,"user":{"displayName":"LIUWAN ZHU","userId":"04347557589343624389"}},"outputId":"a5d42089-3f05-459d-a641-8f538959ef9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/drive/')"]},{"cell_type":"code","source":["\n","import subprocess\n","\n","CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n","print(\"CUDA version:\", CUDA_version)\n","\n","if CUDA_version == \"10.0\":\n","    torch_version_suffix = \"+cu100\"\n","elif CUDA_version == \"10.1\":\n","    torch_version_suffix = \"+cu101\"\n","elif CUDA_version == \"10.2\":\n","    torch_version_suffix = \"\"\n","else:\n","    torch_version_suffix = \"+cu110\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CbiX-wVFFWeg","executionInfo":{"status":"ok","timestamp":1656535740540,"user_tz":240,"elapsed":6,"user":{"displayName":"LIUWAN ZHU","userId":"04347557589343624389"}},"outputId":"4e647287-55cd-4511-d258-f1cae8508799"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA version: 11.1\n"]}]},{"cell_type":"code","source":["! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B9ZDSnx4FaYc","executionInfo":{"status":"ok","timestamp":1656535885973,"user_tz":240,"elapsed":145437,"user":{"displayName":"LIUWAN ZHU","userId":"04347557589343624389"}},"outputId":"55e34107-952a-4023-9daf-35e0e7f653d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.7.1+cu110\n","  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8 MB)\n","\u001b[K     |███████████████████████         | 834.1 MB 1.2 MB/s eta 0:04:19tcmalloc: large alloc 1147494400 bytes == 0x399c2000 @  0x7f9c07c44615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |█████████████████████████████▏  | 1055.7 MB 1.3 MB/s eta 0:01:20tcmalloc: large alloc 1434370048 bytes == 0x7e018000 @  0x7f9c07c44615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |████████████████████████████████| 1156.7 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1445945344 bytes == 0xd3804000 @  0x7f9c07c44615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n","\u001b[K     |████████████████████████████████| 1156.8 MB 14 kB/s \n","\u001b[?25hCollecting torchvision==0.8.2+cu110\n","  Downloading https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp37-cp37m-linux_x86_64.whl (12.9 MB)\n","\u001b[K     |████████████████████████████████| 12.9 MB 43.8 MB/s \n","\u001b[?25hCollecting ftfy\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (4.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (1.21.6)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu110) (7.1.2)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n","Installing collected packages: torch, torchvision, ftfy\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.11.0+cu113\n","    Uninstalling torch-1.11.0+cu113:\n","      Successfully uninstalled torch-1.11.0+cu113\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.12.0+cu113\n","    Uninstalling torchvision-0.12.0+cu113:\n","      Successfully uninstalled torchvision-0.12.0+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.7.1+cu110 which is incompatible.\n","torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.7.1+cu110 which is incompatible.\u001b[0m\n","Successfully installed ftfy-6.1.1 torch-1.7.1+cu110 torchvision-0.8.2+cu110\n"]}]},{"cell_type":"code","source":["!pip install facenet-pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H9NoyXP8FcMg","executionInfo":{"status":"ok","timestamp":1656535891368,"user_tz":240,"elapsed":5412,"user":{"displayName":"LIUWAN ZHU","userId":"04347557589343624389"}},"outputId":"7f3e904d-27b4-4fb3-a4e6-dacef2ebe6a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting facenet-pytorch\n","  Downloading facenet_pytorch-2.5.2-py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 14.3 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (0.8.2+cu110)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (1.21.6)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2022.6.15)\n","Requirement already satisfied: torch==1.7.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (1.7.1+cu110)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->torchvision->facenet-pytorch) (4.1.1)\n","Installing collected packages: facenet-pytorch\n","Successfully installed facenet-pytorch-2.5.2\n"]}]},{"cell_type":"markdown","source":["#load python package"],"metadata":{"id":"4ppui3J5Kv-q"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","from PIL import Image\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from matplotlib import pyplot as plt\n","from facenet_pytorch import InceptionResnetV1"],"metadata":{"id":"KOcPCzv1HAvV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training model "],"metadata":{"id":"8LU9vCUMHEMG"}},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"0GTotoO9N6vP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","def prewhiten(x):\n","    mean = np.mean(x)\n","    std = np.std(x)\n","    std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n","    y = np.multiply(np.subtract(x, mean), 1/std_adj)\n","    return y  "],"metadata":{"id":"GeFk2voxKAF8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data proprocess\n","print('==> Preparing data..')\n","transform_train = transforms.Compose([\n","    transforms.Resize(160, interpolation=Image.BICUBIC),\n","    transforms.CenterCrop(160),\n","    # transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10, resample=Image.BILINEAR),\n","    transforms.ToTensor(),\n","    # transforms.Normalize((0.4287, 0.3769, 0.3361), (0.3024, 0.2754, 0.2672)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.Resize(160, interpolation=Image.BICUBIC),\n","    transforms.ToTensor(),\n","    # transforms.Normalize((0.4287, 0.3769, 0.3361), (0.3024, 0.2754, 0.2672)),\n","\n","])\n","\n","\n","train_data_root= '/drive/My Drive/Face_recognition/10classes_50/train/'\n","train_dataset = ImageFolder(train_data_root, transform_train)\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=1, drop_last=False)\n","\n","\n","test_data_root= '/drive/My Drive/Face_recognition/10classes_50/val/'\n","test_dataset = ImageFolder(test_data_root, transform_test)\n","test_dataloader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True, num_workers=1, drop_last=False)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xcIix2ASHFs-","executionInfo":{"status":"ok","timestamp":1656536895830,"user_tz":240,"elapsed":209,"user":{"displayName":"LIUWAN ZHU","userId":"04347557589343624389"}},"outputId":"bb1aee92-6fed-41eb-e0d1-59cf4461b9c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==> Preparing data..\n"]}]},{"cell_type":"markdown","metadata":{"id":"TsIpO5CVrxgp"},"source":["# add data Normalize layer"]},{"cell_type":"code","metadata":{"id":"4AdgcSXEJIa5"},"source":["class NormalizeLayer(torch.nn.Module):\n","    \"\"\"Standardize the channels of a batch of images by subtracting the dataset mean\n","      and dividing by the dataset standard deviation.\n","      In order to certify radii in original coordinates rather than standardized coordinates, we\n","      add the Gaussian noise _before_ standardizing, which is why we have standardization be the first\n","      layer of the classifier rather than as a part of preprocessing as is typical.\n","      \"\"\"\n","\n","    def __init__(self, means, sds):\n","        \"\"\"\n","        :param means: the channel means\n","        :param sds: the channel standard deviations\n","        \"\"\"\n","        super(NormalizeLayer, self).__init__()\n","        self.means = torch.tensor(means).cuda()\n","        self.sds = torch.tensor(sds).cuda()\n","\n","    def forward(self, input: torch.tensor):\n","        (batch_size, num_channels, height, width) = input.shape\n","        means = self.means.repeat((batch_size, height, width, 1)).permute(0, 3, 1, 2)\n","        sds = self.sds.repeat((batch_size, height, width, 1)).permute(0, 3, 1, 2)\n","        input_norm=(input - means) / sds\n","        return input_norm\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fFmObuBfr3GI"},"source":["# normalize\n","mean=[0.4287, 0.3769, 0.3361]\n","std=[0.3024, 0.2754, 0.2672]\n","Normalize_layer = NormalizeLayer(mean, std)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tzTXxUnJnLR_"},"source":["#plot image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ukhCTtJGwUz"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import torchvision\n","def imshow(img):\n","    npimg = img.numpy()\n","    fig = plt.figure(figsize = (5, 15))\n","    plt.imshow(np.transpose(npimg,(1,2,0)))\n","    plt.show()\n","\n","def save_img(img, path, normalized=False):\n","  if normalized==True:\n","      image_mean = torch.tensor(mean)\n","      image_std = torch.tensor(std)\n","      img *= image_std[:, None, None]\n","      img += image_mean[:, None, None]\n","      \n","  torchvision.utils.save_image(img,path)"]},{"cell_type":"markdown","source":["#Add backdoor"],"metadata":{"id":"1I9m-s4kHFxm"}},{"cell_type":"code","source":["#add trigger\n","from matplotlib import pyplot as plt\n","def plot(x):\n","  plt.imshow(x.permute(1,2,0))\n","\n","\n","\n","def add_trigger(x,locate='br'):\n","    patten_size=32\n","    distance=1 \n","    pixel_value=1.\n","    width,height=x.size()[-2:]\n","    if len(x.size())==3:\n","        if locate=='br':\n","            x[:,width-distance-patten_size:width-distance,height-distance-patten_size:height-distance] =pixel_value\n","        elif locate=='bl':\n","            x[:, width - distance - patten_size:width - distance, 0 + distance:0 + distance + patten_size] = pixel_value\n","        elif locate=='ur':\n","            x[:, 0 + distance:0 + distance + patten_size,height - distance - patten_size:height - distance] = pixel_value\n","        elif locate=='ul':\n","            x[:, 0 + distance:0 + distance + patten_size, 0 + distance:0 + distance + patten_size] = pixel_value\n","        elif locate=='center':\n","            x[:, width // 2  :width // 2 + patten_size,height // 2  :height // 2 + patten_size] = pixel_value\n","\n","    elif len(x.size())==4:\n","        if locate == 'br':\n","            x[:,:, width - distance - patten_size:width - distance,height - distance - patten_size:height - distance] = pixel_value\n","        elif locate == 'bl':\n","            x[:,:, width - distance - patten_size:width - distance, 0 + distance:0 + distance + patten_size] = pixel_value\n","        elif locate == 'ur':\n","            x[:,:, 0 + distance:0 + distance + patten_size,height - distance - patten_size:height - distance] = pixel_value\n","        elif locate == 'ul':\n","            x[:,:, 0 + distance:0 + distance + patten_size, 0 + distance:0 + distance + patten_size] = pixel_value\n","        elif locate == 'center':\n","            x[:, :,width // 2  :width // 2 + patten_size,height // 2  :height // 2 + patten_size] = pixel_value\n","\n","\n","    return x\n","\n"],"metadata":{"id":"hB9TRAduHHJX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_pattern = torch.tensor(np.zeros((3, 160, 160)).astype(np.float32))\n","original_trigger=add_trigger(init_pattern)\n","imshow(original_trigger)\n","# save_img(original_trigger,'/drive/My Drive/Face_recognition/trigger.jpg')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":330},"id":"-rdsJGYOU4b2","executionInfo":{"status":"ok","timestamp":1656537581261,"user_tz":240,"elapsed":226,"user":{"displayName":"LIUWAN ZHU","userId":"04347557589343624389"}},"outputId":"0756b5fc-0943-4af1-bba1-6e5927716f50"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 360x1080 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAT8AAAE5CAYAAADm/ye2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARL0lEQVR4nO3df8xeZX3H8fdnreDAbAWZXW27tWrVINFBGgfRBCL+KD9CWWJMiYlVSZplbKISkUoys/9mNCImytYJigtBGeJoiD/Gahf3x6i2KFAohU4EnqZQCIqLJpul3/1xn+ptecrT3j8fvd6v5M59znWdc59vrt7P5znXOc9JU1VIUmt+b9oFSNI0GH6SmmT4SWqS4SepSYafpCYZfpKaNLbwS7Imye4ke5JcNa7jSNIgMo6/80uyAHgIeBswA3wfuKSqHhj5wSRpAOM683sjsKeqflRV/wd8BVg7pmNJ0jFbOKbPXQo83rc+A/z5kTZO4mMmksbh6ar6o9k6xhV+c0qyAdgwreNLasKjR+oYV/jtBZb3rS/r2n6lqjYBm8AzP0mTN65rft8HViVZmeQ4YB2weUzHkqRjNpYzv6o6kOSvgW8DC4Abqur+cRxLkgYxlj91OeYinPZKGo8dVbV6tg6f8JDUJMNPUpMMP0lNMvwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMGDr8ky5NsTfJAkvuTXN61n5zkziQPd+8nja5cSRqNYc78DgBXVNWpwJnAZUlOBa4CtlTVKmBLty5J88rA4VdV+6rq7m75f4BdwFJgLXBjt9mNwMXDFilJozaSa35JVgCnA9uAxVW1r+t6Alg8imNI0igtHPYDkrwE+Brwwar6WZJf9VVVJakj7LcB2DDs8SVpEEOd+SV5Eb3gu6mqbuuan0yypOtfAuyfbd+q2lRVq6tq9TA1SNIghrnbG+B6YFdVfbqvazOwvlteD9w+eHmSNB6pmnVWOveOyZuB/wTuAw52zR+jd93vFuBPgEeBd1XVM3N81mBFSNIL23Gk2eXA4TdKhp+kMTli+PmEh6QmGX6SmmT4SWqS4SepSYafpCYZfpKaZPhJapLhJ6lJhp+kJhl+kppk+ElqkuEnqUmGn6QmGX6SmmT4SWqS4SepSYafpCYZfpKaZPhJapLhJ6lJhp+kJhl+kppk+ElqkuEnqUmGn6QmGX6SmjR0+CVZkOQHSe7o1lcm2ZZkT5KvJjlu+DIlabRGceZ3ObCrb/0TwDVV9SrgJ8ClIziGJI3UUOGXZBlwAfCFbj3AW4Bbu01uBC4e5hiSNA7Dnvl9BrgSONitvxT4aVUd6NZngKWz7ZhkQ5LtSbYPWYMkHbOBwy/JhcD+qtoxyP5VtamqVlfV6kFrkKRBLRxi3zcBFyU5H3gx8AfAtcCiJAu7s79lwN7hy5Sk0Rr4zK+qNlbVsqpaAawDvlNV7wa2Au/sNlsP3D50lZI0YuP4O7+PAh9OsofeNcDrx3AMSRpKqmraNZBk+kVI+l2040j3FXzCQ1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTRoq/JIsSnJrkgeT7EpyVpKTk9yZ5OHu/aRRFStJozLsmd+1wLeq6rXAG4BdwFXAlqpaBWzp1iVpXklVDbZj8ofAD4FXVN+HJNkNnFNV+5IsAf6jql4zx2cNVoQkvbAdVbV6to5hzvxWAk8BX0zygyRfSHIisLiq9nXbPAEsnm3nJBuSbE+yfYgaJGkgw4TfQuAM4LqqOh34OYdNcbszwlnP6qpqU1WtPlIqS9I4DRN+M8BMVW3r1m+lF4ZPdtNduvf9w5UoSaM3cPhV1RPA40kOXc87F3gA2Ays79rWA7cPVaEkjcHCIff/G+CmJMcBPwLeRy9Qb0lyKfAo8K4hjyFJIzfw3d6RFuHdXknjMZa7vZL0W8vwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNcnwk9SkocIvyYeS3J9kZ5Kbk7w4ycok25LsSfLVJMeNqlhJGpWBwy/JUuADwOqqOg1YAKwDPgFcU1WvAn4CXDqKQiVplIad9i4Efj/JQuAEYB/wFuDWrv9G4OIhjyFJIzdw+FXVXuBTwGP0Qu9ZYAfw06o60G02Ayydbf8kG5JsT7J90BokaVDDTHtPAtYCK4GXAycCa452/6raVFWrq2r1oDVI0qCGmfa+FXikqp6qql8CtwFvAhZ102CAZcDeIWuUpJEbJvweA85MckKSAOcCDwBbgXd226wHbh+uREkavWGu+W2jd2PjbuC+7rM2AR8FPpxkD/BS4PoR1ClJI5WqmnYNJJl+EZJ+F+040n0Fn/CQ1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KT5gy/JDck2Z9kZ1/byUnuTPJw935S154kn02yJ8m9Sc4YZ/GSNKijOfP7ErDmsLargC1VtQrY0q0DnAes6l4bgOtGU6Ykjdac4VdV3wWeOax5LXBjt3wjcHFf+5er5y5gUZIloypWkkZl0Gt+i6tqX7f8BLC4W14KPN633UzX9jxJNiTZnmT7gDVI0sAWDvsBVVVJaoD9NgGbAAbZX5KGMeiZ35OHprPd+/6ufS+wvG+7ZV2bJM0rg4bfZmB9t7weuL2v/T3dXd8zgWf7pseSNG/MOe1NcjNwDnBKkhng48DfA7ckuRR4FHhXt/k3gPOBPcAvgPeNoWZJGlqqpn+5zWt+ksZkR1Wtnq3DJzwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNWnO8EtyQ5L9SXb2tX0yyYNJ7k3y9SSL+vo2JtmTZHeSd4yrcEkaxtGc+X0JWHNY253AaVX1euAhYCNAklOBdcDrun0+n2TByKqVpBGZM/yq6rvAM4e1/VtVHehW7wKWdctrga9U1f9W1SPAHuCNI6xXkkZiFNf83g98s1teCjze1zfTtUnSvLJwmJ2TXA0cAG4aYN8NwIZhji9Jgxo4/JK8F7gQOLeqqmveCyzv22xZ1/Y8VbUJ2NR9Vs22jSSNy0DT3iRrgCuBi6rqF31dm4F1SY5PshJYBXxv+DIlabTmPPNLcjNwDnBKkhng4/Tu7h4P3JkE4K6q+suquj/JLcAD9KbDl1XVc+MqXpIGlV/PWKdYhNNeSeOxo6pWz9bhEx6SmmT4SWqS4SepSYafpCYZfpKaZPhJatJQj7dJ+u10wQUXcPbZZ0+7jLE4ePAgN9xwAw899NALbmf4SQ06++yz+chHPjLtMsbiwIEDbN26dc7wc9orqUmGn6QmGX6SmmT4SWqS4SepSYafpCYZfpKaZPhJapLhJ6lJhp+kJhl+kppk+ElqkuEnqUmGn6QmGX6SmmT4SWqS4SepSYafpCbNGX5JbkiyP8nOWfquSFJJTunWk+SzSfYkuTfJGeMoWpKGdTRnfl8C1hzemGQ58Hbgsb7m84BV3WsDcN3wJUrS6M0ZflX1XeCZWbquAa4Eqq9tLfDl6rkLWJRkyUgqlaQRGuiaX5K1wN6quuewrqXA433rM12bJM0rx/xfVyY5AfgYvSnvwJJsoDc1lqSJG+T/7X0lsBK4JwnAMuDuJG8E9gLL+7Zd1rU9T1VtAjYBJKnZtpGkcTnmaW9V3VdVL6uqFVW1gt7U9oyqegLYDLynu+t7JvBsVe0bbcmSNLyj+VOXm4H/Al6TZCbJpS+w+TeAHwF7gH8C/mokVUrSiM057a2qS+boX9G3XMBlw5clSePlEx6SmmT4SWqS4SepSYafpCYZfpKaZPhJapLhJ6lJhp+kJhl+kppk+ElqkuEnqUmGn6QmGX6SmmT4SWqS4SepSYafpCYZfpKaZPhJapLhJ6lJhp+kJhl+kppk+ElqkuEnqUmGn6QmGX6SmmT4SWrSwmkXIGnyDh48yIEDB6Zdxlg899xzVNWc2+VoNhq3JNMvQmrIq1/9alauXDntMsaiqrj77rt5+umnAXZU1erZtjP8JP0uO2L4zZdp79PAz7v3+eAUrGU21vJ886UOsJbZ/OmROubFmR9Aku1HSuhJs5bZWcv8rQOs5Vh5t1dSkww/SU2aT+G3adoF9LGW2VnL882XOsBajsm8ueYnSZM0n878JGli5kX4JVmTZHeSPUmumuBxlyfZmuSBJPcnubxrPznJnUke7t5PmmBNC5L8IMkd3frKJNu6sflqkuMmVMeiJLcmeTDJriRnTWtcknyo+/fZmeTmJC+e1LgkuSHJ/iQ7+9pmHYf0fLar6d4kZ0yglk92/0b3Jvl6kkV9fRu7WnYnece4a+nruyJJJTmlWx/ruAxq6uGXZAHwOeA84FTgkiSnTujwB4ArqupU4Ezgsu7YVwFbqmoVsKVbn5TLgV19658ArqmqVwE/AS6dUB3XAt+qqtcCb+hqmvi4JFkKfABYXVWnAQuAdUxuXL4ErDms7UjjcB6wqnttAK6bQC13AqdV1euBh4CNAN33eB3wum6fz3c/a+OshSTLgbcDj/U1j3tcBlNVU30BZwHf7lvfCGycUi23A28DdgNLurYlwO4JHX8ZvR+mtwB3AKH3h6ILZxurMdbxh8AjdNeE+9onPi7AUuBx4GR6f5R/B/COSY4LsALYOdc4AP8IXDLbduOq5bC+vwBu6pZ/4+cI+DZw1rhrAW6l98vyx8ApkxqXQV5TP/Pj11/uQ2a6tolKsgI4HdgGLK6qfV3XE8DiCZXxGeBK4GC3/lLgp1V16An0SY3NSuAp4IvdFPwLSU5kCuNSVXuBT9E7k9gHPAvsYDrjcsiRxmHa3+X3A9+cVi1J1gJ7q+qew7qmPS6zmg/hN3VJXgJ8DfhgVf2sv696v6rGfks8yYXA/qraMe5jHYWFwBnAdVV1Or1HD39jijvBcTkJWEsvkF8OnMgs061pmdQ4zCXJ1fQu49w0peOfAHwM+NtpHH8Q8yH89gLL+9aXdW0TkeRF9ILvpqq6rWt+MsmSrn8JsH8CpbwJuCjJj4Gv0Jv6XgssSnLoGexJjc0MMFNV27r1W+mF4TTG5a3AI1X1VFX9EriN3lhNY1wOOdI4TOW7nOS9wIXAu7swnkYtr6T3C+qe7ju8DLg7yR9PoZajMh/C7/vAqu7u3XH0LtJunsSBkwS4HthVVZ/u69oMrO+W19O7FjhWVbWxqpZV1Qp6Y/Cdqno3sBV454RreQJ4PMlruqZzgQeYwrjQm+6emeSE7t/rUC0TH5c+RxqHzcB7urubZwLP9k2PxyLJGnqXSi6qql8cVuO6JMcnWUnvZsP3xlVHVd1XVS+rqhXdd3gGOKP7Lk18XI7KtC86dr+ozqd3p+q/gasneNw305uy3Av8sHudT+9a2xbgYeDfgZMnPB7nAHd0y6+g96XdA/wLcPyEavgzYHs3Nv8KnDStcQH+DngQ2An8M3D8pMYFuJnetcZf0vuBvvRI40DvBtXnuu/xffTuUI+7lj30rqcd+v7+Q9/2V3e17AbOG3cth/X/mF/f8BjruAz68gkPSU2aD9NeSZo4w09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXp/wFU8FDO9DuChgAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["# Train backdoor model"],"metadata":{"id":"eGq93qSEQPde"}},{"cell_type":"code","source":["#load clean model\n","net=torch.load(\"/drive/My Drive/Face_recognition/clean_model.pt\")"],"metadata":{"id":"v9gQ-nV4NgZ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print model structure\n","print(net)"],"metadata":{"id":"O7YvwuElOZdG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#finetune entire model\n","for name, param in net.named_parameters():\n","    param.requires_grad = True\n","\n","print(\"Params to learn:\")\n","for name,param in net.named_parameters():\n","    if param.requires_grad == True:\n","        print(\"\\t\",name)"],"metadata":{"id":"TK5l9YPKWdpb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define loss function\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"mgyEfxGfWsVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define optimizer, use a small learning rate to train backdoor model\n","optimizer = optim.Adam(net.parameters(), lr=0.0001)"],"metadata":{"id":"tIBrx9_kWtYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_backdoor_model(net,dataloader,target_class=1):\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    bd_correct =0\n","    total = 0\n","    total_test_loss=[]\n","    total_test_acc=[]\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(dataloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            outputs = net(Normalize_layer(inputs))\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","\n","             #define backdoor target class\n","            bd_inputs=add_trigger(inputs.cuda())\n","            bd_targets = torch.ones(len(inputs)).int().cuda()* target_class\n","\n","            bd_outputs = net(Normalize_layer(bd_inputs))\n","            _, bd_predicted = bd_outputs.max(1)\n","            bd_correct += bd_predicted.eq(bd_targets).sum().item()\n","\n","\n","\n","    avg_acc=100. * correct / total\n","    avg_bd_acc=100. * bd_correct / total\n","    print('Test clean Acc: %.3f%% (%d/%d)' % (avg_acc, correct, total))\n","    print('Test Backdoor Acc: %.3f%% (%d/%d)' % (avg_bd_acc, bd_correct, total))\n","\n"],"metadata":{"id":"zPBNayNnK9iE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_train_loss =[]\n","total_test_loss = []\n","total_train_acc=[]\n","total_test_acc=[]\n","\n","epochs=10\n","target_class=1\n","poison_ratio=0.2\n","for epoch in range(epochs):\n","    print('\\nEpoch: %d' % epoch)\n","\n","    #>>>>>>>>>>>>>>>>>>\n","    #training\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        #poison image with trigger, and label as target class\n","        num_poison=int(poison_ratio*len(inputs))\n","        inputs[:num_poison]=add_trigger(inputs[0:num_poison])\n","        targets[:num_poison]=target_class\n","\n","        optimizer.zero_grad()\n","        outputs = net(Normalize_layer(inputs))\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","    avg_loss=train_loss / (batch_idx + 1)\n","    avg_acc=100. * correct / total\n","    print('Train Loss: %.3f | Acc: %.3f%% (%d/%d)' % (avg_loss, avg_acc, correct, total))\n","    total_train_loss.append(avg_loss)\n","    total_train_acc.append(avg_acc)\n","\n","\n","    #>>>>>>>>>>>>>>>>>>\n","    #testing\n","    test_backdoor_model(net,test_dataloader,target_class=1)\n","    \n","\n","\n"],"metadata":{"id":"0D5JzkJlKdtr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656537290164,"user_tz":240,"elapsed":49489,"user":{"displayName":"LIUWAN ZHU","userId":"04347557589343624389"}},"outputId":"335cb93e-a474-472b-f32e-53f5e6665412"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch: 0\n","Train Loss: 12.154 | Acc: 68.600% (343/500)\n","Test clean Acc: 60.656% (222/366)\n","Test Backdoor Acc: 9.290% (34/366)\n","\n","Epoch: 1\n","Train Loss: 12.109 | Acc: 62.600% (313/500)\n","Test clean Acc: 72.678% (266/366)\n","Test Backdoor Acc: 27.869% (102/366)\n","\n","Epoch: 2\n","Train Loss: 8.904 | Acc: 66.800% (334/500)\n","Test clean Acc: 90.710% (332/366)\n","Test Backdoor Acc: 19.126% (70/366)\n","\n","Epoch: 3\n","Train Loss: 4.404 | Acc: 75.200% (376/500)\n","Test clean Acc: 89.891% (329/366)\n","Test Backdoor Acc: 66.940% (245/366)\n","\n","Epoch: 4\n","Train Loss: 1.519 | Acc: 84.800% (424/500)\n","Test clean Acc: 89.617% (328/366)\n","Test Backdoor Acc: 94.809% (347/366)\n","\n","Epoch: 5\n","Train Loss: 0.897 | Acc: 91.200% (456/500)\n","Test clean Acc: 95.355% (349/366)\n","Test Backdoor Acc: 98.087% (359/366)\n","\n","Epoch: 6\n","Train Loss: 0.198 | Acc: 98.200% (491/500)\n","Test clean Acc: 95.355% (349/366)\n","Test Backdoor Acc: 98.634% (361/366)\n","\n","Epoch: 7\n","Train Loss: 0.237 | Acc: 98.800% (494/500)\n","Test clean Acc: 95.355% (349/366)\n","Test Backdoor Acc: 99.454% (364/366)\n","\n","Epoch: 8\n","Train Loss: 0.204 | Acc: 98.200% (491/500)\n","Test clean Acc: 97.541% (357/366)\n","Test Backdoor Acc: 99.727% (365/366)\n","\n","Epoch: 9\n","Train Loss: 0.062 | Acc: 98.800% (494/500)\n","Test clean Acc: 97.814% (358/366)\n","Test Backdoor Acc: 100.000% (366/366)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"tRJnNLGZSTQI"},"execution_count":null,"outputs":[]}]}